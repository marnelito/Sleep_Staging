{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sleep staging on the Sleep Physionet dataset\n",
    "\n",
    "## Sleep staging\n",
    "\n",
    "Sleep staging is the process of identifying the sleep stage someone is in by analyzing their EEG and other physiological signals. Sleep recordings are traditionally divided into 30-s windows, and one of five categories (\"stages\") is attributed to each window:\n",
    "\n",
    "1. W: wakefulness\n",
    "2. N1: light sleep\n",
    "3. N2: deeper sleep\n",
    "4. N3: deep sleep\n",
    "5. R: rapid eye movement\n",
    "\n",
    "Sleep staging usually relies on capturing changes in the spectral properties of the EEG as well as transient events (e.g., sleep spindles, k-complexes, slow waves, etc.) that occur under the different sleep stages.\n",
    "\n",
    "Thus, our objective is to create a Neural Network able to accurately classify the different sleep stages without the help of an expert. We will train a convolutional neural network (ConvNet) to perform sleep staging on unseen raw EEG. We will use the [Sleep Physionet](https://physionet.org/content/sleep-edfx/1.0.0/) dataset, which contains 153 overnight sleep recordings from 78 individuals. These recordings were manually staged by sleep experts, providing us with the required classification targets to train and evaluate our ConvNet on.\n",
    "\n",
    "## Steps\n",
    "\n",
    "This notebook is divided into the following sections:\n",
    "\n",
    "0. [Set up environment](#0.-Setting-up-the-environment)\n",
    "1. [Load data](#1.-Loading-data)\n",
    "2. [Preprocess data (filter, window)](#2.-Preprocessing-raw-data)\n",
    "    1. [Frequency filtering](#2.1.-Filtering)\n",
    "    2. [Artifact Handling](#2.2.-Artifact-handling)\n",
    "        1. [Automatic and Tunable Artifact Removal Algorithm (**ATAR**)](#2.2.1.-ATAR)\n",
    "    3. [Extracting Epochs](#2.3.-Extracting-Epochs)\n",
    "3. [Make splits](#3.-Making-train,-valid-and-test-splits)\n",
    "4. [Convert the datasets as Pandas Dataframes](#4.-Dataset-to-Dataframe)\n",
    "5. [Resampling the Data](#5.-Resampling-the-data)\n",
    "6. [Detection of anomalies/outliers](#6.-Anomaly-detection)\n",
    "7. [Visualizing the Data](#7.-Visualizing-the-Data)\n",
    "8. [Data Augmentation](#8.-Data-Augmentation)\n",
    "    1. [Brain Waves](#8.1.-Brain-Waves)\n",
    "    2. [Pearson Cross Correlation Coefficient](#8.2.-Pearson-Cross-Correlation-Coefficient)\n",
    "9. [Dataframe to Dataset](#9.-Dataframe-to-Dataset)\n",
    "10. [Create model](#10.-Creating-the-neural-network)\n",
    "11. [Train and monitor](#11.-Train-and-monitor-network)\n",
    "12. [Visualize results](#12.-Visualizing-results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. Setting up the environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install torch  #should already be installed on colab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identify whether a CUDA-enabled GPU is available\n",
    "import torch\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    print('CUDA-enabled GPU found. Training should be faster.')\n",
    "else:\n",
    "    print('No GPU found. Training will be carried out on CPU, which might be '\n",
    "          'slower.\\n\\nIf running on Google Colab, you can request a GPU runtime by'\n",
    "          ' clicking\\n`Runtime/Change runtime type` in the top bar menu, then '\n",
    "          'selecting \\'GPU\\'\\nunder \\'Hardware accelerator\\'.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages for colab\n",
    "%pip install mne\n",
    "%pip install torch\n",
    "%pip install matplotlib\n",
    "%pip install scikit-learn\n",
    "%pip install pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import general modules\n",
    "import os\n",
    "import copy\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Choosing the method**\n",
    "\n",
    "This code is based on :\n",
    "> Chambon, S., Galtier, M. N., Arnal, P. J., Wainrib, G., & Gramfort, A. (2018). A deep learning architecture for temporal sleep stage classification using multivariate and multimodal time series. IEEE Transactions on Neural Systems and Rehabilitation Engineering, 26(4), 758-769.\n",
    "\n",
    "We implemented a few new different methods in order to improve the classification. To vizualize the impact of these methods here you can choose whether or not you want to apply them. The methods we implemented are :\n",
    "\n",
    "1. Artifact handling using ATAR and/or Widrow filtering\n",
    "2. Anomaly detection with Autoencoders\n",
    "3. Resampling the data using different methods : SMOTE, ADASYN, random undersampling and oversampling...\n",
    "4. Feature extraction for better performances\n",
    "5. Generative Adversial Network to create new data to train the model\n",
    "\n",
    "**In the cell below change the variable to True if you want to use this method**\n",
    "\n",
    "If you want to resample the data (resampling==True), do not forget to specify the resampling method you wish to use.\n",
    "For the ADASYN method you also need to choose undersampling or oversampling to be applied after because it does not give balanced classes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Artifact handling\n",
    "atar = False\n",
    "# Choose between : \"soft\", \"linAtten\" and \"elim\"\n",
    "optmode = \"elim\"\n",
    "beta_value = 0.3\n",
    "\n",
    "# Resampling the Data\n",
    "resampling = True\n",
    "# Choose from : \"SMOTETomek\", \"ADASYN\", \"random_under_sampling\", \"random_over_sampling\"\n",
    "resampling_method = \"SMOTETomek\"\n",
    "# for adasyn only (the sampling method that follows ADASYN because ADASYN alone does not give balanced classes)\n",
    "# 0 --> undersampling\n",
    "# 1 --> oversampling\n",
    "follow_adasyn = 0\n",
    "\n",
    "# Anomaly detection\n",
    "anomaly_detection = False\n",
    "percentage_of_anomalies = 1 # %\n",
    "\n",
    "# Data augmentation\n",
    "additional_features = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Loading data\n",
    "\n",
    "We start by loading the raw EEG recordings from the Sleep Physionet dataset. MNE-Python already contains a function `fetch_data` which downloads the recordings locally. We then need to read each file from the disk.\n",
    "\n",
    "To make the first pass through this tutorial faster, we only load a part of the entire Sleep Physionet dataset (30 recordings out of 153). Once you are able to run the whole tutorial and are ready to work on improving the performance of the model, you can try loading more subjects and recordings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mne\n",
    "from mne.datasets.sleep_physionet.age import fetch_data\n",
    "\n",
    "mne.set_log_level('ERROR')  # To avoid flooding the cell outputs with messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subjects = range(30)\n",
    "recordings = [1]\n",
    "\n",
    "# To load all subjects and recordings, uncomment the next line\n",
    "# subjects, recordings = range(83), [1, 2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fnames = fetch_data(subjects=subjects, recording=recordings, on_missing='warn')\n",
    "\n",
    "\"\"\" Get paths to local copies of PhysioNet Polysomnography dataset files. This will fetch data from the publicly available\n",
    "    subjects from PhysioNet's study of age effects on sleep in healthy subjects 12. This corresponds to a subset of\n",
    "    153 recordings from 37 males and 41 females that were 25-101 years old at the time of the recordings.\n",
    "\n",
    "Parameters\n",
    "----------\n",
    "subjects : list of int\n",
    "    The subjects to use. Can be in the range of 0-82 (inclusive), however the following subjects are not available: 39, 68, 69, 78 and 79.\n",
    "\n",
    "recording : list of int\n",
    "    The night recording indices. Valid values are : [1], [2], or [1, 2].\n",
    "    The following recordings are not available: recording 1 for subject 36 and 52, and recording 2 for subject 13.\n",
    "\n",
    "path : None | str\n",
    "    Location of where to look for the PhysioNet data storing location.\n",
    "    If None, the environment variable or config parameter PHYSIONET_SLEEP_PATH is used.\n",
    "    If it doesn't exist, the “~/mne_data” directory is used.\n",
    "    If the Polysomnography dataset is not found under the given path, the data will be automatically downloaded to the specified folder.\n",
    "\n",
    "on_missing : 'raise' | 'warn' | 'ignore'\n",
    "    What to do if one or several recordings are not available. Valid keys are 'raise' | 'warn' | 'ignore'.\n",
    "    Default is 'error'. If on_missing is 'warn' it will proceed but warn, if 'ignore' it will proceed silently.\n",
    "\n",
    "Returns\n",
    "-------\n",
    "fnames : \n",
    "    list of paths to the .edf files  \n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_sleep_physionet_raw(raw_fname, annot_fname, load_eeg_only=True, \n",
    "                             crop_wake_mins=30):\n",
    "    \"\"\"Load a recording from the Sleep Physionet dataset.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    raw_fname : str\n",
    "        Path to the .edf file containing the raw data.\n",
    "    annot_fname : str\n",
    "        Path to the annotation file.\n",
    "    load_eeg_only : bool\n",
    "        If True, only keep EEG channels and discard other modalities \n",
    "        (speeds up loading).\n",
    "    crop_wake_mins : float\n",
    "        Number of minutes of wake events before and after sleep events.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    mne.io.Raw :\n",
    "        Raw object containing the EEG and annotations.        \n",
    "    \"\"\"\n",
    "    mapping = {'EOG horizontal': 'eog',\n",
    "               'Resp oro-nasal': 'misc',\n",
    "               'EMG submental': 'misc',\n",
    "               'Temp rectal': 'misc',\n",
    "               'Event marker': 'misc'}\n",
    "    exclude = mapping.keys() if load_eeg_only else ()\n",
    "    \n",
    "    raw = mne.io.read_raw_edf(raw_fname, exclude=exclude)\n",
    "    annots = mne.read_annotations(annot_fname) \n",
    "    raw.set_annotations(annots, emit_warning=False)\n",
    "    if not load_eeg_only:\n",
    "        raw.set_channel_types(mapping)\n",
    "    \n",
    "    if crop_wake_mins > 0:  # Cut start and end Wake periods\n",
    "        # Find first and last sleep stages\n",
    "        mask = [x[-1] in ['1', '2', '3', '4', 'R'] \n",
    "                for x in annots.description]\n",
    "        sleep_event_inds = np.where(mask)[0]\n",
    "\n",
    "        # Crop raw\n",
    "        tmin = annots[int(sleep_event_inds[0])]['onset'] - \\\n",
    "               crop_wake_mins * 60\n",
    "        tmax = annots[int(sleep_event_inds[-1])]['onset'] + \\\n",
    "               crop_wake_mins * 60\n",
    "        raw.crop(tmin=tmin, tmax=tmax)\n",
    "    \n",
    "    # Rename EEG channels\n",
    "    ch_names = {i: i.replace('EEG ', '') \n",
    "                for i in raw.ch_names if 'EEG' in i}\n",
    "    mne.rename_channels(raw.info, ch_names)\n",
    "    \n",
    "    # Save subject and recording information in raw.info\n",
    "    basename = os.path.basename(raw_fname)\n",
    "    subj_nb, rec_nb = int(basename[3:5]), int(basename[5])\n",
    "    raw.info['subject_info'] = {'id': subj_nb, 'rec_id': rec_nb}\n",
    "   \n",
    "    return raw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load recordings\n",
    "raws = [load_sleep_physionet_raw(f[0], f[1]) for f in fnames]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot a recording as a sanity check\n",
    "raws[0].plot();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Preprocessing raw data\n",
    "\n",
    "Next, we need to preprocess the raw data. Here, we use a simple filtering step, followed by the deletion of artifacts and then extraction of 30-s windows."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1. Filtering\n",
    "\n",
    "Sleep EEG data has most of its relevant information below 30 Hz. Therefore, to mitigate the impact of higher frequency noise, we apply a lowpass filter with cutoff frequency of 30 Hz to our recordings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "l_freq, h_freq = None, 30\n",
    "\n",
    "for raw in raws:\n",
    "    raw.load_data().filter(l_freq, h_freq)  # filtering happens in-place"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot power or amplitude spectrum. Separate plots are drawn for each channel type. When the data have been processed with a bandpass, lowpass or highpass filter, dashed lines (╎) indicate the boundaries of the filter. The line noise frequency is also indicated with a dashed line (⋮). If average=False, the plot will be interactive, and click-dragging on the spectrum will generate a scalp topography plot for the chosen frequency range in a new figure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the power spectrum of a recording as sanity check\n",
    "raws[0].plot_psd();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the power spectral density plot, we can see that our filter has indeed cut off most of the power above 30 Hz."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2. Artifact Handling\n",
    "\n",
    "Artifacts in EEG signals are disturbances or interferences that are not related to brain electrical activity but can be induced by various sources. The removal and management of these artifacts are crucial for obtaining high-quality EEG recordings and for interpreting the data accurately. We will try different methods that will be hopefully added in the future but for now let's focus on the Automatic and Tunable Artifact Removal Algorithm known as **ATAR**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.2.1. ATAR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install spkit\n",
    "import spkit as sp\n",
    "sp.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def atar_artifact(raw, mode=optmode, beta=0.01, wavelet='db8') :\n",
    "    \"\"\"Apply the Adaptive Thresholding for Artifact Rejection (ATAR) method to remove artifacts from EEG data\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    raw : mne.io.Raw\n",
    "        Raw EEG data in MNE format\n",
    "    optmode : str, optional\n",
    "        Optimization mode for ATAR ('soft' or 'hard', default is 'soft')\n",
    "    beta : float, optional\n",
    "        Parameter controlling the trade-off between sensitivity and specificity in ATAR (default is 0.01)\n",
    "    wavelet : str, optional\n",
    "        Wavelet function to be used in ATAR (default is 'db8')\n",
    "    Returns\n",
    "    -------\n",
    "    mne.io.Raw\n",
    "        Raw EEG data after applying the ATAR artifact removal method\n",
    "    \"\"\"\n",
    "    for i in range(len(raw.info['ch_names'])) :\n",
    "        X = raw.get_data(raw.info['ch_names'][i])\n",
    "        X_atar =  sp.eeg.ATAR(X.ravel(), beta=beta, OptMode=mode, wv=wavelet, verbose=0,)\n",
    "        raw._data[i, :] = X_atar\n",
    "        \n",
    "    return raw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time # use of timer to monitor the function\n",
    "\n",
    "if atar == True :\n",
    "    for i, raw in enumerate(raws) :\n",
    "        start_time = time.time()\n",
    "        raws[i] = atar_artifact(raw, beta=beta_value)\n",
    "        end_time = time.time()\n",
    "        elapsed_time = end_time - start_time\n",
    "        print(f\"Itération {i+1}: Temps écoulé {elapsed_time} secondes\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3. Extracting Epochs\n",
    "\n",
    "Before proceeding to extracting 30-s windows (also called *epochs*) from the filtered data, we define a few functions that we will need:\n",
    "\n",
    "The function extract_epochs returns all epochs as a 3D array using the built-in method epochs.get_data and the number of the events associated with each epoch with epochs.events[:, 2] - 1 (from 0 to 4 instead of 1 to 5).\n",
    "Let's have a look at all the different functions that are used here :\n",
    "\n",
    "**<u>mne.events_from_annotations(`raw`, `event_id='auto'`)</u>** : Get <font color='blue'>events</font> and <font color='blue'>`event_id`</font> from an Annotations object.\n",
    "\n",
    "**raw : *instance of `Raw`*** --> The raw data for which Annotations are defined.\n",
    "\n",
    "**event_id : *`dict`* | *`callable()`* | *`None`* | *`auto`*** \n",
    "Can be :\n",
    "\n",
    "* **dict** : map descriptions (keys) to integer event codes (values). Only the descriptions present will be mapped, others will be ignored.\n",
    "\n",
    "* **callable** : must take a string input and return an integer event code, or return `None` to ignore the event.\n",
    "\n",
    "* None: Map descriptions to unique integer values based on their sorted order.\n",
    "\n",
    "* **‘auto’ (default)** : prefer a raw-format-specific parser:\n",
    "\n",
    "    * Brainvision: map stimulus events to their integer part; response events to integer part + 1000; optic events to integer part + 2000; ‘SyncStatus/Sync On’ to 99998; ‘New Segment/’ to 99999; all others like `None` with an offset of 10000.\n",
    "\n",
    "    * Other raw formats: Behaves like None.\n",
    "\n",
    "**<u>mne.pick_types(*info, eeg, eog*)</u>** : Pick channels by type and names.\n",
    "\n",
    "**info : *`mne.Info`*** : The **`mne.Info`** object with information about the sensors and methods of measurement.\n",
    "\n",
    "**eeg : *`bool`*** : If True include EEG channels.\n",
    "\n",
    "**eog : *`bool`*** : If True include EOG channels.\n",
    "\n",
    "**<u>epochs.get_data()</u>** : Get all epochs as a 3D array."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_epochs(raw, chunk_duration=30.):\n",
    "    \"\"\"Extract non-overlapping epochs from raw data.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    raw : mne.io.Raw\n",
    "        Raw data object to be windowed.\n",
    "    chunk_duration : float\n",
    "        Length of a window.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    np.ndarray\n",
    "        Epoched data, of shape (n_epochs, n_channels, n_times).\n",
    "    np.ndarray\n",
    "        Event identifiers for each epoch, shape (n_epochs,).\n",
    "    \"\"\"\n",
    "    annotation_desc_2_event_id = {\n",
    "        'Sleep stage W': 1,\n",
    "        'Sleep stage 1': 2,\n",
    "        'Sleep stage 2': 3,\n",
    "        'Sleep stage 3': 4,\n",
    "        'Sleep stage 4': 4,\n",
    "        'Sleep stage R': 5}\n",
    "\n",
    "    events, _ = mne.events_from_annotations(\n",
    "        raw, event_id=annotation_desc_2_event_id, \n",
    "        chunk_duration=chunk_duration)\n",
    "\n",
    "    # create a new event_id that unifies stages 3 and 4\n",
    "    event_id = {\n",
    "        'Sleep stage W': 1,\n",
    "        'Sleep stage 1': 2,\n",
    "        'Sleep stage 2': 3,\n",
    "        'Sleep stage 3/4': 4,\n",
    "        'Sleep stage R': 5}\n",
    "\n",
    "    tmax = 30. - 1. / raw.info['sfreq']  # tmax in included\n",
    "    picks = mne.pick_types(raw.info, eeg=True, eog=True)\n",
    "    epochs = mne.Epochs(raw=raw, events=events, picks=picks, preload=True,\n",
    "                        event_id=event_id, tmin=0., tmax=tmax, baseline=None)\n",
    "    \n",
    "    return epochs.get_data(), epochs.events[:, 2] - 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, ConcatDataset\n",
    "\n",
    "\n",
    "class EpochsDataset(Dataset):\n",
    "    \"\"\"Class to expose an MNE Epochs object as PyTorch dataset.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    epochs_data : np.ndarray\n",
    "        The epochs data, shape (n_epochs, n_channels, n_times).\n",
    "    epochs_labels : np.ndarray\n",
    "        The epochs labels, shape (n_epochs,)\n",
    "    subj_nb: None | int\n",
    "        Subject number.\n",
    "    rec_nb: None | int\n",
    "        Recording number.\n",
    "    transform : callable | None\n",
    "        The function to eventually apply to each epoch\n",
    "        for preprocessing (e.g. scaling). Defaults to None.\n",
    "    \"\"\"\n",
    "    def __init__(self, epochs_data, epochs_labels, subj_nb=None, \n",
    "                 rec_nb=None, transform=None):\n",
    "        assert len(epochs_data) == len(epochs_labels)\n",
    "        self.epochs_data = epochs_data\n",
    "        self.epochs_labels = epochs_labels\n",
    "        self.subj_nb = subj_nb\n",
    "        self.rec_nb = rec_nb\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.epochs_labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        X, y = self.epochs_data[idx], self.epochs_labels[idx]\n",
    "        if self.transform is not None:\n",
    "            X = self.transform(X)\n",
    "        X = torch.as_tensor(X[None, ...])\n",
    "        return X, y\n",
    "    \n",
    "\n",
    "def scale(X):\n",
    "    \"\"\"Standard scaling of data along the last dimension.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    X : array, shape (n_channels, n_times)\n",
    "        The input signals.\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    X_t : array, shape (n_channels, n_times)\n",
    "        The scaled signals.\n",
    "    \"\"\"\n",
    "    X -= np.mean(X, axis=1, keepdims=True)\n",
    "    return X / np.std(X, axis=1, keepdims=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now extract windows from each recording, and wrap them into Pytorch datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply windowing and move to pytorch dataset\n",
    "all_datasets = [EpochsDataset(*extract_epochs(raw), subj_nb=raw.info['subject_info']['id'], \n",
    "                              rec_nb=raw.info['subject_info']['rec_id'], transform=scale) \n",
    "                for raw in raws]\n",
    "\n",
    "# Concatenate into a single dataset\n",
    "dataset = ConcatDataset(all_datasets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You might have noticed we added a scaling transform to the datasets. This scaling makes sure each EEG channel in each 30-s window has a mean of 0 and a standard deviation of 1. This will help the neural network when training."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Making train, valid and test splits\n",
    "\n",
    "Now that we have our preprocessed and windowed data, we can split it into the different sets that we will need: (1) the **training set** is used to learn the parameters of our ConvNet, (2) the **validation set** is used to monitor the training process and decide when to stop it, and (3) the **test set** is used to provide an estimate of the generalization performance of our model.\n",
    "\n",
    "Here, we keep recording 1 of subjects 0-9 for testing, and split the remaining recordings into training and validation sets.\n",
    "\n",
    "We define the following functions to perform the split:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import LeavePGroupsOut\n",
    "\n",
    "\n",
    "def pick_recordings(dataset, subj_rec_nbs):\n",
    "    \"\"\"Pick recordings using subject and recording numbers.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    dataset : ConcatDataset\n",
    "        The dataset to pick recordings from.        \n",
    "    subj_rec_nbs : list of tuples\n",
    "        List of pairs (subj_nb, rec_nb) to use in split.\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    ConcatDataset\n",
    "        The picked recordings.\n",
    "    ConcatDataset | None\n",
    "        The remaining recordings. None if all recordings from \n",
    "        `dataset` were picked.\n",
    "    \"\"\"\n",
    "    pick_idx = list()\n",
    "    for subj_nb, rec_nb in subj_rec_nbs:\n",
    "        for i, ds in enumerate(dataset.datasets):\n",
    "            if (ds.subj_nb == subj_nb) and (ds.rec_nb == rec_nb):\n",
    "                pick_idx.append(i)\n",
    "                \n",
    "    remaining_idx = np.setdiff1d(\n",
    "        range(len(dataset.datasets)), pick_idx)\n",
    "\n",
    "    pick_ds = ConcatDataset([dataset.datasets[i] for i in pick_idx])\n",
    "    if len(remaining_idx) > 0:\n",
    "        remaining_ds = ConcatDataset(\n",
    "            [dataset.datasets[i] for i in remaining_idx])\n",
    "    else:\n",
    "        remaining_ds = None\n",
    "    \n",
    "    return pick_ds, remaining_ds\n",
    "    \n",
    "\n",
    "def train_test_split(dataset, n_groups, split_by='subj_nb'):\n",
    "    \"\"\"Split dataset into train and test keeping n_groups out in test.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    dataset : ConcatDataset\n",
    "        The dataset to split.\n",
    "    n_groups : int\n",
    "        The number of groups to leave out.\n",
    "    split_by : 'subj_nb' | 'rec_nb'\n",
    "        Property to use to split dataset.\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    ConcatDataset\n",
    "        The training data.\n",
    "    ConcatDataset\n",
    "        The testing data.\n",
    "    \"\"\"\n",
    "    groups = [getattr(ds, split_by) for ds in dataset.datasets]\n",
    "    train_idx, test_idx = next(\n",
    "        LeavePGroupsOut(n_groups).split(X=groups, groups=groups))\n",
    "\n",
    "    train_ds = ConcatDataset([dataset.datasets[i] for i in train_idx])\n",
    "    test_ds = ConcatDataset([dataset.datasets[i] for i in test_idx])\n",
    "        \n",
    "    return train_ds, test_ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We seed the random number generators to make our splits reproducible\n",
    "torch.manual_seed(87)\n",
    "np.random.seed(87)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use recording 1 of subjects 0-9 as test set\n",
    "test_recs = [(subj_nb, rec_nb)  # DO NOT CHANGE! This is a fixed set.\n",
    "             for subj_nb, rec_nb in zip(range(10), [1] * 10)]\n",
    "test_ds, train_ds = pick_recordings(dataset, test_recs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split remaining recordings into training and validation sets\n",
    "n_subjects_valid = max(1, int(len(train_ds.datasets) * 0.2))\n",
    "train_ds, valid_ds = train_test_split(train_ds, n_subjects_valid, split_by='subj_nb')\n",
    "\n",
    "print('Number of examples in each set:')\n",
    "print(f'Training: {len(train_ds)}')\n",
    "print(f'Validation: {len(valid_ds)}')\n",
    "print(f'Test: {len(test_ds)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Dataset to Dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Now we need to convert the ConcatDataset into a Pandas.Dataframe in order to work with it.**\n",
    "\n",
    "The function `ConcatDataset_to_Dataframe` does this for us and gives us a dataframe with all the epochs and their label, not sorted. If we want to have multiple dataframe based on the label of the epoch (the 'label' column of each dataframe will have the same value for all rows), we can use the function `DataFrame_to_Dictionnary` which gives us a dictionnary with a nulber of elements equivalent the the number of conditions (label) or sleep stages in our case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ConcatDataset_to_Dataframe(concatdataset) :\n",
    "    \"\"\"Converts a ConcatDataset into a pandas.DataFrame\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    concatdataset : ConcatDataset\n",
    "        ConcatDataset from which to retrieve the data\n",
    "    Returns\n",
    "    -------\n",
    "    df : pandas.DataFrame\n",
    "        DataFrame with 3 columns (['data'], ['label'], ['dataset_idx'])\n",
    "        and number of rows = number of datasets in concatdataset\n",
    "    \"\"\"\n",
    "    # We create a list of datasets using the concatdataset\n",
    "    dataset_list = []\n",
    "    for i in range(len(concatdataset.datasets)) :\n",
    "        dataset_list.append(concatdataset.datasets[i])\n",
    "\n",
    "    # Creating an empty list to store the data\n",
    "    data_list = []\n",
    "    # We iterate through the datasets, collecting the data and labels into the data_list\n",
    "    for dataset_idx in range(len(dataset_list)) :\n",
    "        for j in range(len(dataset_list[dataset_idx])) :\n",
    "            # Data and label of the dataset's element\n",
    "            data_tensor  = dataset_list[dataset_idx][j][0][0]\n",
    "            label = dataset_list[dataset_idx][j][1]\n",
    "            # We convert the tensor (Dataset[dataset_idx][0]) and the label (Dataset[dataset_idx][1]) into np.array\n",
    "            data_array = data_tensor.numpy()\n",
    "\n",
    "            # we append everything to the data_list\n",
    "            data_list.append((data_array, label, dataset_idx))\n",
    "\n",
    "    # We create a pandas DataFrame from the list of tuples (data, label, dataset_idx).\n",
    "    df = pd.DataFrame(data_list, columns=[\"data\", \"label\", \"dataset_idx\"])\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Converting the train dataset into a dataframe**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = ConcatDataset_to_Dataframe(train_ds)\n",
    "df_train.head() #to check"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Resampling the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We notice that the classes are imbalanced, i.e., there are a lot more of some classes than others. N2, especially, is more prevalent than the other sleep stages during the night:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classes_mapping = {0: 'W', 1: 'N1', 2: 'N2', 3: 'N3', 4: 'R'}\n",
    "y_train = pd.Series([y for _, y in train_ds]).map(classes_mapping)\n",
    "ax = y_train.value_counts().plot(kind='barh')\n",
    "ax.set_xlabel('Number of training examples');\n",
    "ax.set_ylabel('Sleep stage');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One way to account for this imbalance during training is to give more weight to examples from rarer classes when computing the loss but since it is not the most efficient method we will try and implment some new ones.\n",
    "\n",
    "The function `sampling_for_imbalanced_data` is designed to achieve this. You can choose between different method to do so\n",
    "\n",
    "(We will leave the weights calculation after the resampling part to check if everything went well)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "from imblearn.combine import SMOTETomek\n",
    "\n",
    "\n",
    "def count_occurences(dataframe, column_name) :\n",
    "    \"\"\"Count the occurrences of each element in the specified column\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    dataframe : pandas.DataFrame\n",
    "        DataFrame from which occurrences are counted\n",
    "    column_name : str\n",
    "        Name of the column for which occurrences are counted\n",
    "        Example: 'label'\n",
    "    Returns\n",
    "    -------\n",
    "    count : dict\n",
    "        Dictionary containing the names of elements in column_name and their occurrences\n",
    "    \"\"\"\n",
    "    counts = dataframe[column_name].value_counts().to_dict()\n",
    "    \n",
    "    return counts\n",
    "\n",
    "\n",
    "def DataFrame_to_Dictionnary(dataframe, mapping) :\n",
    "    \"\"\"Converts a DataFrame into a Dictionary of DataFrames\n",
    "    The DataFrames are separated based on their labels\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    dataframe : pandas.DataFrame\n",
    "        DataFrame to be split and organized into the dictionary\n",
    "    mapping : dict\n",
    "        Dictionary used to map each number in the 'label' column\n",
    "        of the DataFrame to a corresponding letter\n",
    "        Example sleep stage: mapping = {0: 'W', 1: 'N1', 2: 'N2', 3: 'N3', 4: 'R'}\n",
    "    Returns\n",
    "    -------\n",
    "    dataframes_by_label_letter : dict\n",
    "        Dictionary containing multiple DataFrames organized under different keys\n",
    "        Each 'label' column in the DataFrames has a unique value given by the dictionary key\n",
    "    \"\"\"\n",
    "    grouped = dataframe.groupby(\"label\")\n",
    "\n",
    "    # We create a list of individual DataFrames\n",
    "    dataframes_by_label = []\n",
    "\n",
    "    # We iterate through the different groups and add the new DataFrames to our list\n",
    "    for label, group in grouped:\n",
    "        dataframes_by_label.append(group.copy())\n",
    "    \n",
    "    dataframes_by_label_letter = {}\n",
    "    # We iterate through the DataFrames and assign keys to them based on the mapping dictionary\n",
    "    for df_temp in enumerate(dataframes_by_label):\n",
    "        df_temp = df_temp.reset_index(drop=True)\n",
    "        label = list(set(df_temp['label']))[0]  # Obtaining the unique label from the DataFrame\n",
    "        lettre_label = mapping[label]\n",
    "        cle_dictionnaire = f\"Condition_{lettre_label}\"\n",
    "        dataframes_by_label_letter[cle_dictionnaire] = df_temp\n",
    "        \n",
    "    return dataframes_by_label_letter\n",
    "\n",
    "\n",
    "def sampling_for_imbalanced_data(df, methode, mapping):\n",
    "    \"\"\"Sample unbalanced data to improve NN training\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    dataframe : pandas.DataFrame\n",
    "        DataFrame from which we want to sample and balance the data\n",
    "    method : str\n",
    "        name of the method we want to use to balance the data\n",
    "    mapping : dict\n",
    "        Dictionary used to map the labels in the dataframe to a specific letter\n",
    "    Returns\n",
    "    -------\n",
    "    pandas.DataFrame\n",
    "        DataFrame containing an equal number of occurrences for each 'label'\n",
    "    \"\"\"\n",
    "    # Appliquer la fonction pour aplatir chaque élément de la colonne \"data\"\n",
    "    df = df.reset_index(drop=True)\n",
    "    \n",
    "    if methode == \"random_under_sampling\" :\n",
    "        df_new = []\n",
    "        # On regarde quel est la condition qui possède le moins de valeurs (clé du dictionnaire)\n",
    "        min_value_key = min(count_occurences(df, 'label'), key=count_occurences(df, 'label').get)\n",
    "        # On regarde quelle est cette valeur\n",
    "        min_value = count_occurences(df, 'label')[min_value_key]\n",
    "        min_value_key = mapping.get(min_value_key, \"cette valeur n'est pas dans le dictionnaire\")\n",
    "        # Nouveau dictionnaire des conditions sans celle dont on se sert pour undersampler\n",
    "        # Copie du dictionnaire original\n",
    "        new_mapping = mapping.copy()\n",
    "        # Valeur à supprimer\n",
    "        value_to_remove = min_value_key\n",
    "\n",
    "        # Recherche de la clé correspondante\n",
    "        key_to_remove = None\n",
    "        for key, value in new_mapping.items() :\n",
    "            if value == value_to_remove:\n",
    "                key_to_remove = key\n",
    "                break\n",
    "\n",
    "        # Suppression de la clé et de sa valeur correspondante\n",
    "        if key_to_remove is not None :\n",
    "            del new_mapping[key_to_remove]\n",
    "        else:\n",
    "            print(f\"La valeur '{value_to_remove}' n'a pas été trouvée dans le dictionnaire.\")\n",
    "\n",
    "        # Pour chaque sleep stage qui contient trop d'élément, on en enlève en parcourant le nouveau dictionnaire\n",
    "        j = 0\n",
    "        for i in new_mapping :\n",
    "            df_new.append(DataFrame_to_Dictionnary(df, mapping)[\"Condition_{}\".format(new_mapping[i])].sample(min_value))\n",
    "            df_new[j] = df_new[j].reset_index(drop=True)\n",
    "            j += 1\n",
    "\n",
    "        df_new.append(DataFrame_to_Dictionnary(df, mapping)[\"Condition_{}\".format(min_value_key)])\n",
    "        df_under = pd.concat(df_new, ignore_index=True)\n",
    "\n",
    "        return df_under\n",
    "    \n",
    "    \n",
    "    elif methode == \"random_over_sampling\" :\n",
    "        df_new = []\n",
    "        # On regarde quel est la condition qui possède le plus de valeurs (clé du dictionnaire)\n",
    "        max_value_key = max(count_occurences(df, 'label'), key=count_occurences(df, 'label').get)\n",
    "        \n",
    "        # On regarde quelle est cette valeur\n",
    "        max_value = count_occurences(df, 'label')[max_value_key]\n",
    "        max_value_key = mapping.get(max_value_key, \"cette valeur n'est pas dans le dictionnaire\")\n",
    "        # Nouveau dictionnaire des Conditions sans le stage dont on se sert pour oversampler\n",
    "        # Copie du dictionnaire original\n",
    "        new_mapping = mapping.copy()\n",
    "        # Valeur à supprimer\n",
    "        value_to_remove = max_value_key\n",
    "\n",
    "        # Recherche de la clé correspondante\n",
    "        key_to_remove = None\n",
    "        for key, value in new_mapping.items() :\n",
    "            if value == value_to_remove:\n",
    "                key_to_remove = key\n",
    "                break\n",
    "\n",
    "        # Suppression de la clé et de sa valeur correspondante\n",
    "        if key_to_remove is not None :\n",
    "            del new_mapping[key_to_remove]\n",
    "        else:\n",
    "            print(f\"La valeur '{value_to_remove}' n'a pas été trouvée dans le dictionnaire.\")\n",
    "\n",
    "        # Pour chaque Condition qui contient trop d'élément, on en enlève en parcourant le nouveau dictionnaire\n",
    "        j = 0\n",
    "        for i in new_mapping :\n",
    "            df_new.append(DataFrame_to_Dictionnary(df, mapping)[\"Condition_{}\".format(new_mapping[i])].sample(max_value, replace=True))\n",
    "            df_new[j] = df_new[j].reset_index(drop=True)\n",
    "            j += 1\n",
    "\n",
    "        df_new.append(DataFrame_to_Dictionnary(df, mapping)[\"Condition_{}\".format(max_value_key)])\n",
    "        df_over = pd.concat(df_new, ignore_index=True)\n",
    "\n",
    "        return df_over\n",
    "    \n",
    "\n",
    "    elif methode == \"SMOTETomek\" :\n",
    "        df_copy = df.copy()\n",
    "\n",
    "        X = []\n",
    "        for i in range(len(df_copy['data'])) :\n",
    "            X.append(df_copy['data'][i].ravel())\n",
    "\n",
    "        print(\"shape X : \", np.shape(X))\n",
    "        y = df_copy['label']\n",
    "\n",
    "        smt = SMOTETomek(sampling_strategy='auto')\n",
    "        X_smt, y_smt = smt.fit_resample(X, y) # Resampling\n",
    "        print(\"shape X smt : \", np.shape(X_smt))\n",
    "\n",
    "        X_final = []\n",
    "        # Parcourez chaque ligne de \"data\" et divisez-la en segments\n",
    "        for i in range(len(X_smt)):\n",
    "            X_final.append((np.array(X_smt[i])).reshape((2, 3000)))\n",
    "\n",
    "        dataset_idx = df_copy['dataset_idx'].ravel().tolist()\n",
    "        last_idx = dataset_idx[-1]\n",
    "        dataset_idx.extend([last_idx + 1] * (len(y_smt) - len(dataset_idx)))\n",
    "\n",
    "        df_balanced = pd.DataFrame({'data': X_final, 'label': y_smt, 'dataset_idx': dataset_idx})\n",
    "\n",
    "        return df_balanced\n",
    "    \n",
    "\n",
    "    elif methode == \"ADASYN\" :\n",
    "        df_copy = df.copy()\n",
    "\n",
    "        X = []\n",
    "        for i in range(len(df_copy['data'])) :\n",
    "            X.append((df_copy['data'][i]).ravel())\n",
    "\n",
    "        print(\"shape X : \", np.shape(X))\n",
    "        y = df_copy['label']\n",
    "\n",
    "        from imblearn.over_sampling import ADASYN\n",
    "        adasyn = ADASYN(sampling_strategy='auto', n_neighbors=5, random_state=42)\n",
    "\n",
    "        X_ada, y_ada = adasyn.fit_resample(X, y) # Resampling\n",
    "        print(\"shape X ada : \", np.shape(X_ada))\n",
    "\n",
    "        X_final = []\n",
    "        # On Parcourt chaque ligne de \"data\" et on la divise en segments\n",
    "        for i in range(len(X_ada)):\n",
    "            X_final.append((np.array(X_ada[i])).reshape((2, 3000)))\n",
    "\n",
    "        dataset_idx = df_copy['dataset_idx'].ravel().tolist()\n",
    "        last_idx = dataset_idx[-1]\n",
    "        dataset_idx.extend([last_idx + 1] * ((len(y_ada) - len(dataset_idx))))\n",
    "\n",
    "        df_balanced = pd.DataFrame({'data': X_final, 'label': y_ada, 'dataset_idx': dataset_idx})\n",
    "\n",
    "        return df_balanced\n",
    "    \n",
    "\n",
    "    else :\n",
    "        print(\"Cette méthode de sampling n'est pas disponible\")\n",
    "        return 0\n",
    "\n",
    "\n",
    "def plot_label_occurences(dataframe, method) :\n",
    "    \"\"\"Plot the occurrences of each label in a DataFrame using a custom color palette\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    dataframe : pandas.DataFrame\n",
    "        DataFrame containing the data with a 'label' column\n",
    "    method : str\n",
    "        Resampling Method name or identifier used for the plot title\n",
    "    Returns\n",
    "    -------\n",
    "    None\n",
    "        Displays a bar plot showing the number of occurrences for each unique label in the DataFrame\n",
    "    \"\"\"\n",
    "    # On compte les labels uniques de la DataFrame\n",
    "    labels_uniques = dataframe['label'].unique()\n",
    "    # Création d'une palette de couleurs personnalisée avec une couleur différente pour chaque label\n",
    "    couleurs_palette = sns.color_palette(\"husl\", len(labels_uniques))\n",
    "\n",
    "    # On compte le nombre d'occurrences de chaque label\n",
    "    label_counts = dataframe['label'].value_counts()\n",
    "\n",
    "    # Création de l'histogramme\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    label_counts.plot(kind='bar', color=couleurs_palette)\n",
    "    plt.title('{} : Nombre d\\'occurrences par label'.format(method))\n",
    "    plt.xlabel('Labels')\n",
    "    plt.ylabel('Nombre d\\'occurrences')\n",
    "    plt.xticks(rotation=0)\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if resampling==True :\n",
    "    # On compte les éléments de chaque classe\n",
    "    count_class = y_train.value_counts().to_dict()\n",
    "    print(count_class)\n",
    "    print(classes_mapping)\n",
    "\n",
    "    df_train = sampling_for_imbalanced_data(df_train, resampling_method, classes_mapping)\n",
    "\n",
    "    if resampling_method==\"ADASYN\" :\n",
    "        if follow_adasyn == 0 :\n",
    "            df_train = sampling_for_imbalanced_data(df_train, \"random_under_sampling\", classes_mapping)\n",
    "        elif follow_adasyn == 1 :\n",
    "            df_train = sampling_for_imbalanced_data(df_train, \"random_over_sampling\", classes_mapping)\n",
    "\n",
    "    plot_label_occurences(df_train, resampling_method)\n",
    "\n",
    "else :\n",
    "    plot_label_occurences(df_train, \"No resampling\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Anomaly detection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By using the reconstruction error as anomaly score, we can try and spot anomalies in the classes and even bad channels. For a simple example, we will just use a threshold, if the anomaly score is higher, we will mark the epoch as an anomaly and delete it form the dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "\n",
    "\n",
    "def anomaly_detection_with_AE(df, n_epochs=50, encoding_dimension=8, percentage_of_anomalies=1) :\n",
    "    \"\"\"Perform anomaly detection using an Autoencoder neural network on a DataFrame with time-series data\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    df : pandas.DataFrame\n",
    "        DataFrame containing a 'data' column with time-series data and a 'label' column with class labels\n",
    "    n_epochs : int, optional\n",
    "        Number of training epochs for the Autoencoder (default is 50)\n",
    "    encoding_dimension : int, optional\n",
    "        Dimension of the latent representation in the Autoencoder (default is 8)\n",
    "    percentage_of_anomalies : int, optional\n",
    "        Percentage of anomalies in the data for setting the prediction loss threshold (default is 2)\n",
    "    Returns\n",
    "    -------\n",
    "    pandas.DataFrame\n",
    "        DataFrame with filtered data, excluding outliers identified by the Autoencoder\n",
    "    \"\"\"\n",
    "    df_length = len(df['data'])\n",
    "    # Check if some of the values from the dataframe are NaN (that would raise an error)*\n",
    "    # If so, we delete them\n",
    "    has_nan = df['data'].apply(lambda x: np.isnan(x).any() if isinstance(x, np.ndarray) else False)\n",
    "    if has_nan.any():\n",
    "        df = df.loc[~has_nan]\n",
    "        print(f\"Il y a des données manquantes, données supprimées : {df_length - len(df['data'])}\")\n",
    "\n",
    "    df = df.reset_index(drop=True)\n",
    "        \n",
    "    X = []\n",
    "    for i in range(df_length) :\n",
    "        X.append(df['data'][i]).ravel()\n",
    "\n",
    "    y = df['label']\n",
    "    # Normalisation\n",
    "    scaler = StandardScaler()\n",
    "    X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "    # Créer le modèle de l'autoencodeur\n",
    "    input_dim = X_scaled.shape[1]  # Dimension des données d'entrée\n",
    "    encoding_dim = encoding_dimension  # Dimension de la représentation latente\n",
    "\n",
    "    # Encodeur\n",
    "    input_layer = keras.layers.Input(shape=(input_dim,))\n",
    "    encoder = tf.keras.Sequential([\n",
    "        keras.layers.Dense(encoding_dim*4, activation='relu', kernel_initializer='he_normal'),\n",
    "        keras.layers.Dense(encoding_dim*2, activation='relu', kernel_initializer='he_normal'),\n",
    "        keras.layers.Dense(encoding_dim, activation='relu', kernel_initializer='he_normal')])(input_layer)\n",
    "    encoder_model = keras.models.Model(inputs=input_layer, outputs=encoder)\n",
    "\n",
    "    # Décodeur\n",
    "    decoder_input = keras.layers.Input(shape=(encoding_dim,))\n",
    "    decoder = tf.keras.Sequential([\n",
    "        keras.layers.Dense(encoding_dim*2, activation='relu', kernel_initializer='he_normal'),\n",
    "        keras.layers.Dense(encoding_dim*4, activation='relu', kernel_initializer='he_normal'),\n",
    "        keras.layers.Dense(input_dim, activation='sigmoid', kernel_initializer='he_normal')])(decoder_input)\n",
    "    decoder_model = keras.models.Model(inputs=decoder_input, outputs=decoder)\n",
    "\n",
    "    # Autoencodeur complet\n",
    "    autoencoder_input = keras.layers.Input(shape=(input_dim,))\n",
    "    encoded = encoder_model(autoencoder_input)\n",
    "    decoded = decoder_model(encoded)\n",
    "    autoencoder = keras.models.Model(inputs=autoencoder_input, outputs=decoded)\n",
    "\n",
    "    # Compiler le modèle\n",
    "    autoencoder.compile(optimizer=\"adam\", loss=\"mean_squared_error\")\n",
    "\n",
    "    # Entraîner l'autoencodeur\n",
    "    autoencoder.fit(X_scaled, X_scaled, epochs=n_epochs, batch_size=64, shuffle=True)\n",
    "\n",
    "    prediction = autoencoder.predict(X_scaled)\n",
    "    # Get the mean absolute error between actual and reconstruction/prediction\n",
    "    prediction_loss = tf.keras.losses.mae(prediction, X_scaled)\n",
    "    # Check the prediction loss threshold for 2% of outliers\n",
    "    loss_threshold = np.percentile(prediction_loss, 100 - percentage_of_anomalies)\n",
    "    print(f'The prediction loss threshold for {percentage_of_anomalies}% of outliers is {loss_threshold:.2f}')\n",
    "    # Visualize the threshold\n",
    "    sns.histplot(prediction_loss, bins=30, alpha=0.8)\n",
    "    palette = sns.color_palette()\n",
    "    # Tracer la ligne verticale avec la même couleur\n",
    "    plt.axvline(x=loss_threshold, color=palette[0])\n",
    "\n",
    "    outlier_indices = np.where(prediction_loss > loss_threshold)[0]\n",
    "    # Now, the `outlier_indices` variable contains the indices of elements in X_scaled\n",
    "    # that have prediction_loss above the loss_threshold.\n",
    "    print(\"number of outliers :\", len(outlier_indices))\n",
    "    \n",
    "    df_final = df.drop(outlier_indices)\n",
    "\n",
    "    X_new = [X[i] for i in range(len(X)) if i not in outlier_indices]\n",
    "    y = y[:-len(outlier_indices)]\n",
    "    \n",
    "    X_final = []\n",
    "    for i in range(len(X_new)):\n",
    "        X_final.append((np.array(X_new[i])).reshape((2, int(len(df['data'][0][0])))))\n",
    "    \n",
    "    df_final['data'] = X_final\n",
    "    \n",
    "    return df_final"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To start with, we will delete 1% of the data from each class based on the threshold loss\n",
    "\n",
    "You can modify this value as you wish when you call the function `anomaly_detection_with_AE`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if anomaly_detection==True :\n",
    "    df_dict = DataFrame_to_Dictionnary(df_train, classes_mapping)\n",
    "\n",
    "    df_new = []\n",
    "    for i in range(len(df_dict)) :\n",
    "        df = df_dict[\"Condition_{}\".format(classes_mapping[i])]\n",
    "        df_final = anomaly_detection_with_AE(df, n_epochs=50, encoding_dimension=16, percentage_of_anomalies=percentage_of_anomalies)\n",
    "        df_new.append(df_final)\n",
    "    df_train = pd.concat(df_new, ignore_index=True)\n",
    "    df_train = df_train.reset_index(drop=True)\n",
    "    del df_new"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Visualizing the Data\n",
    "\n",
    "This section uses PCA decomposition to plot the multidimensionnal data on the 2D space in order to better understand why the model has such a hard time differencitating between certain classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "\n",
    "def pca_conversion(df, n_components=2) :\n",
    "    \"\"\"Perform PCA (Principal Component Analysis) on a DataFrame's 'data' column and return a new DataFrame\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    df : pandas.DataFrame\n",
    "        DataFrame containing a 'data' column with multidimensional arrays or numerical values\n",
    "        and a 'label' column containing integers\n",
    "    n_components : int, optional\n",
    "        Number of principal components to retain (default is 2)\n",
    "    Returns\n",
    "    -------\n",
    "    pandas.DataFrame\n",
    "        DataFrame with transformed 'data' column using PCA and an additional 'label' column\n",
    "    \"\"\"\n",
    "    # On crée une instance PCA avec deux composantes principales\n",
    "    pca = PCA(n_components)\n",
    "    # On reset les index de la dataframe pour éviter les erreurs d'indexation\n",
    "    df = df.reset_index(drop=True)\n",
    "    # Cette partie du code sert à tranformer chaque élément de la colonne 'data'\n",
    "    # qui peuvent être des arrays multidimensionnels ou des floats ou int en une liste \n",
    "    # cette liste est alors rangée dans X à la place équivalente à la ligne de la dataframe\n",
    "    # Cette étape doit bien sur être adaptée en fonction de la nature des données à convertir\n",
    "    # Dans cet exemple, les données de data étaient des array de taille (2,3000)\n",
    "    # On en a donc fait une liste qui est donc de taille (1,6000), exploitable par la méthode pca\n",
    "    y = df['label']\n",
    "    X = []\n",
    "    for i in range(len(df['data'])) :\n",
    "        X.append(df['data'][i]).ravel()\n",
    "\n",
    "    X_pca = pca.fit_transform(X)\n",
    "    df_pca = pd.DataFrame({'data': X_pca.tolist(), 'label': y})\n",
    "\n",
    "    return df_pca\n",
    "\n",
    "\n",
    "def plot_2d_space(X, y, classes_mapping=None, n_samples=500, title='Classes', color_map='coolwarm'):\n",
    "    \"\"\"Plot a 2D representation of data points with distinct colors and markers for each class\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    X : array-like\n",
    "        2D array or list containing the data points to be plotted\n",
    "    y : array-like\n",
    "        1D array or list containing class labels corresponding to each data point in X\n",
    "    classes_mapping : dict, optional\n",
    "        Dictionary mapping class labels to human-readable class names (default is None)\n",
    "    n_samples : int, optional\n",
    "        Maximum number of samples to be plotted for each class (default is 500)\n",
    "    title : str, optional\n",
    "        Title of the plot (default is 'Classes')\n",
    "    color_map : str, optional\n",
    "        Colormap name for generating distinct colors for each class (default is 'coolwarm')\n",
    "    Returns\n",
    "    -------\n",
    "    None\n",
    "        Displays a 2D scatter plot of the data points with distinct colors and markers for each class\n",
    "    \"\"\"\n",
    "    nouvelle_largeur = 800\n",
    "    nouvelle_hauteur = 600\n",
    "    dpi = 80\n",
    "    plt.figure(figsize=(nouvelle_largeur / dpi, nouvelle_hauteur / dpi))\n",
    "\n",
    "    # Obtenir la liste des étiquettes uniques\n",
    "    unique_labels = np.unique(y)\n",
    "    \n",
    "    # Utiliser une palette de couleurs pour générer des couleurs distinctes\n",
    "    colors = plt.get_cmap(color_map)(np.linspace(0, 1, len(unique_labels)))\n",
    "\n",
    "    # Utiliser une liste de marqueurs distincts\n",
    "    markers = ['o', 's', 'v', 'D', 'p', '*', 'H', 'X', '8', '>', '<', '^', 'd', '|', '_']\n",
    "\n",
    "    for i, label in enumerate(unique_labels):\n",
    "        class_indices = (y == label)             \n",
    "        class_X = [X[j] for j in range(len(X)) if class_indices[j]]\n",
    "        class_X = np.array(class_X)  # Convert the list to a NumPy array\n",
    "        if len(class_X) > n_samples:\n",
    "            class_X = class_X[:n_samples]\n",
    "        plt.scatter(class_X[:, 0], class_X[:, 1], c=[colors[i]], marker=markers[i], label=classes_mapping[label])\n",
    "\n",
    "    plt.legend(loc='upper right')\n",
    "    plt.title(title)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We first plot all the classes combined on the same graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pca = pca_conversion(df_train)\n",
    "plot_2d_space(df_pca['data'], df_pca['label'], classes_mapping, n_samples=(len(df_train//5)), title='Data in the 2D space (after PCA conversion)')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we plot them separately"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_dict = DataFrame_to_Dictionnary(df_pca, classes_mapping)\n",
    "\n",
    "for i in range(len(df_dict)) :\n",
    "    plot_2d_space(df_dict[\"Condition_{}\".format(classes_mapping[i])]['data'], df_dict[\"Condition_{}\".format(classes_mapping[i])]['label'], classes_mapping, n_samples=(len(df_train//5)), title='class {}'.format(classes_mapping[i]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Data Augmentation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.1. **Brain Waves**\n",
    "\n",
    "There are several types of brain waves, classified based on their frequency and amplitude. The primary categories of brain waves are:\n",
    "\n",
    "1. Delta Waves (0.5-4 Hz): Delta waves are the slowest brain waves and are associated with deep sleep. They have the highest amplitude and are often seen in infants and during stages of deep sleep in adults.\n",
    "\n",
    "2. Theta Waves (4-8 Hz): Theta waves are present during light sleep, deep relaxation, and the early stages of meditation. They are also associated with daydreaming and creative thinking.\n",
    "\n",
    "3. Alpha Waves (8-13 Hz): Alpha waves are dominant during relaxed wakefulness and are often seen when the eyes are closed but the individual is awake and not actively processing information. They are associated with a state of calm and are sometimes referred to as \"idle\" brain waves.\n",
    "\n",
    "4. Beta Waves (13-30 Hz): Beta waves are associated with active, alert, and focused mental activity. They are prominent during wakefulness and cognitive tasks, such as problem-solving and decision-making.\n",
    "\n",
    "5. Gamma Waves (30-100 Hz): Gamma waves are the fastest brain waves and are associated with higher cognitive functions, such as perception, problem-solving, and consciousness. They are also involved in the binding of different sensory inputs into a unified perception.\n",
    "\n",
    "These brain waves are not strictly limited to specific states of consciousness, and their presence can overlap. For example, during certain activities, you may observe a mix of alpha, beta, and theta waves."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.fft import fft\n",
    "\n",
    "def calculate_psd(epoch, sampling_frequency=100):\n",
    "    \"\"\"Calculate the Power Spectral Density (PSD) of a given epoch using Fast Fourier Transform (FFT)\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    epoch : array-like\n",
    "        One-dimensional array or list containing the epoch data\n",
    "    sampling_frequency : int, optional\n",
    "        Sampling frequency of the data (default is 100)\n",
    "    Returns\n",
    "    -------\n",
    "    np.ndarray\n",
    "        One-dimensional NumPy array representing the power spectral density of the epoch\n",
    "    \"\"\"\n",
    "    # Apply Fast Fourier Transform\n",
    "    fft_result = fft(epoch)\n",
    "    \n",
    "    # Calculate power spectral density\n",
    "    psd = np.abs(fft_result) ** 2\n",
    "    \n",
    "    # Normalize PSD by the number of data points and sampling frequency\n",
    "    psd = psd / (len(epoch) * sampling_frequency)\n",
    "    \n",
    "    # Return the PSD\n",
    "    return psd\n",
    "\n",
    "\n",
    "def calculate_band_power(epoch, sampling_frequency, freq_range):\n",
    "    \"\"\"Calculate the spectral power in a given frequency band for a given epoch\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    epoch : array-like\n",
    "        One-dimensional array or list containing the epoch data\n",
    "    sampling_frequency : int\n",
    "        Sampling frequency of the data\n",
    "    freq_range : tuple\n",
    "        Frequency range for which the power should be calculated (e.g., (low_freq, high_freq))\n",
    "    Returns\n",
    "    -------\n",
    "    float\n",
    "        Value representing the spectral power in the specified frequency band for the given epoch\n",
    "    \"\"\"\n",
    "    psd = calculate_psd(epoch, sampling_frequency)\n",
    "\n",
    "    freq = np.fft.fftfreq(len(epoch), d=1/sampling_frequency)\n",
    "\n",
    "    # Find indices corresponding to the frequency range\n",
    "    indices = np.where((freq >= freq_range[0]) & (freq <= freq_range[1]))[0]\n",
    "    \n",
    "    # Sum the power within the specified frequency range\n",
    "    band_power = np.sum(psd[indices])\n",
    "    \n",
    "    return band_power\n",
    "\n",
    "\n",
    "def brainwaves_df(df, sampling_frequency=100) : \n",
    "    \"\"\"Calculate and append brainwave power features to a DataFrame containing EEG data\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    df : pandas.DataFrame\n",
    "        DataFrame containing a 'data' column with EEG data\n",
    "    sampling_frequency : int, optional\n",
    "        Sampling frequency of the EEG data (default is 100)\n",
    "    Returns\n",
    "    -------\n",
    "    pandas.DataFrame\n",
    "        DataFrame with appended columns for delta, theta, alpha, beta, and gamma brainwave power\n",
    "    \"\"\" \n",
    "    # Define brainwave frequency ranges\n",
    "    delta_range = (0.5, 4)\n",
    "    theta_range = (4, 8)\n",
    "    alpha_range = (8, 13)\n",
    "    beta_range = (13, 30)\n",
    "    gamma_range = (30, 100)\n",
    "\n",
    "    data = []\n",
    "\n",
    "    for i in range (len(df['data'])) :\n",
    "        delta_power = np.zeros((len(df['data'][0]), 1))\n",
    "        theta_power = np.zeros((len(df['data'][0]), 1))\n",
    "        alpha_power = np.zeros((len(df['data'][0]), 1))\n",
    "        beta_power = np.zeros((len(df['data'][0]), 1))\n",
    "        gamma_power = np.zeros((len(df['data'][0]), 1))\n",
    "\n",
    "        for j in range (len(df['data'][0])) :\n",
    "\n",
    "            epoch = df['data'][i][j]\n",
    "\n",
    "            # Calculate band powers for each brainwave frequency range\n",
    "            delta_power[j] = calculate_band_power(epoch, sampling_frequency, delta_range)\n",
    "            theta_power[j] = calculate_band_power(epoch, sampling_frequency, theta_range)\n",
    "            alpha_power[j] = calculate_band_power(epoch, sampling_frequency, alpha_range)\n",
    "            beta_power[j] = calculate_band_power(epoch, sampling_frequency, beta_range)\n",
    "            gamma_power[j] = calculate_band_power(epoch, sampling_frequency, gamma_range)\n",
    "    \n",
    "        # Append the results to the data list\n",
    "        data.append([delta_power, theta_power, alpha_power, beta_power, gamma_power])\n",
    "\n",
    "    # Create a DataFrame\n",
    "    column_names = ['Delta', 'Theta', 'Alpha', 'Beta', 'Gamma']\n",
    "    df_waves = pd.DataFrame(data, columns=column_names)\n",
    "\n",
    "    df = df.join(df_waves)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if additional_features == True :\n",
    "    df_train = brainwaves_df(df_train, sampling_frequency=100)\n",
    "    df_train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.2. **Pearson Cross-Correlation Coefficient**\n",
    "\n",
    "The Pearson cross-correlation coefficient, often simply referred to as \"cross-correlation\" or \"correlation,\" measures the linear similarity between two signals (or time series). It is a statistical measure indicating the extent to which variations in two signals are linearly synchronized. It can be beneficial in our case because we only have two EEG channels per recording."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cor_coef(df) :\n",
    "    \"\"\"Calculate the Pearson Cross-Correlation Coefficient for pairs of time-series data in a DataFrame\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    df : pandas.DataFrame\n",
    "        DataFrame containing a 'data' column with pairs of time-series data\n",
    "    Returns\n",
    "    -------\n",
    "    pandas.DataFrame\n",
    "        DataFrame with an added column for the Pearson Cross-Correlation Coefficient\n",
    "    \"\"\"\n",
    "    data = []\n",
    "\n",
    "    for i in range (len(df['data'])) :  \n",
    "        \n",
    "        correlation = np.corrcoef(df['data'][i][0], df['data'][i][1])[0, 1]\n",
    "        data.append(correlation)\n",
    "\n",
    "    df_cor_coef = pd.DataFrame(data, columns=['Pearson Cross-Correlation Coefficient'])\n",
    "    df = df.join(df_cor_coef)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if additional_features == True :\n",
    "    df_train = cor_coef(df_train)\n",
    "    df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also need to add additional features to the valid and test dataset if we chose `additional_features=True`. To do this we convert them into dataframes, add the features, and then wrap them into datasets using the DataFrame_to_ConcatDataset method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if additional_features == True :\n",
    "    valid_df = ConcatDataset_to_Dataframe(valid_ds)\n",
    "    test_df = ConcatDataset_to_Dataframe(test_ds)\n",
    "    valid_df = brainwaves_df(valid_df, sampling_frequency=100)\n",
    "    test_df = brainwaves_df(test_df, sampling_frequency=100)\n",
    "    valid_df = cor_coef(valid_df)\n",
    "    test_df = cor_coef(test_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Dataframe to Dataset\n",
    "After working with the data in the form of a Dataframe, we need to convert it back to a Dataset in order to use it as an input for our neural network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if additional_features == False :\n",
    "    def DataFrame_to_ConcatDataset(df):\n",
    "        \"\"\"Converts a pandas.DataFrame into a torch.utils.data.dataset.ConcatDataset\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        df : pandas.DataFrame\n",
    "            DataFrame with columns ['data'], ['label'], ['dataset_idx']\n",
    "            \n",
    "        Returns\n",
    "        -------\n",
    "        concatdataset : torch.utils.data.dataset.ConcatDataset\n",
    "            ConcatDataset with datasets containing data and labels from the DataFrame\n",
    "        \"\"\"    \n",
    "        unique_dataset_indices = df['dataset_idx'].unique()\n",
    "        datasets = []\n",
    "\n",
    "        for dataset_idx in unique_dataset_indices:\n",
    "            subset_df = df[df['dataset_idx'] == dataset_idx]\n",
    "\n",
    "            data = [torch.from_numpy(np.array(d)) for d in subset_df['data']]\n",
    "            label = subset_df['label'].values\n",
    "\n",
    "            dataset = EpochsDataset(data, label)\n",
    "            datasets.append(dataset)\n",
    "\n",
    "        concatdataset = ConcatDataset(datasets)\n",
    "        \n",
    "        return concatdataset\n",
    "    \n",
    "else :\n",
    "    class EpochsDatasetv2(Dataset):\n",
    "        \"\"\"Class to expose an MNE Epochs object with additional features as a PyTorch dataset.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        epochs_data : np.ndarray\n",
    "            The epochs data, shape (n_epochs, n_channels, n_times).\n",
    "        additional_features: Dict[str, List[torch.Tensor]]\n",
    "            A dictionary containing the additional features for each channel.\n",
    "            The keys are the feature names, and the values are lists of torch tensors.\n",
    "        epochs_labels : np.ndarray\n",
    "            The epochs labels, shape (n_epochs,)\n",
    "        subj_nb: None | int\n",
    "            Subject number.\n",
    "        rec_nb: None | int\n",
    "            Recording number.\n",
    "        transform : callable | None\n",
    "            The function to eventually apply to each epoch for preprocessing (e.g. scaling). Defaults to None.\n",
    "        \"\"\"\n",
    "        def __init__(self, epochs_data, additional_features, epochs_labels, subj_nb=None,\n",
    "                    rec_nb=None, transform=None):\n",
    "            assert len(epochs_data) == len(epochs_labels)\n",
    "            self.epochs_data = epochs_data\n",
    "            self.additional_features = additional_features\n",
    "            self.epochs_labels = epochs_labels\n",
    "            self.subj_nb = subj_nb\n",
    "            self.rec_nb = rec_nb\n",
    "            self.transform = transform\n",
    "\n",
    "        def __len__(self):\n",
    "            return len(self.epochs_labels)\n",
    "\n",
    "        def __getitem__(self, idx):\n",
    "            X, y = self.epochs_data[idx], self.epochs_labels[idx]\n",
    "            additional_X = {feature_name: feature_values[idx] for feature_name, feature_values in self.additional_features.items()}\n",
    "\n",
    "            if self.transform is not None:\n",
    "                X = self.transform(X)\n",
    "\n",
    "            # Convert to torch tensors\n",
    "            X = torch.as_tensor(X[None, ...])\n",
    "            additional_X = {feature_name: torch.as_tensor(values) for feature_name, values in additional_X.items()}\n",
    "\n",
    "            return {'eeg_data': X, 'additional_features': additional_X, 'label': y}\n",
    "        \n",
    "\n",
    "    def DataFrame_to_ConcatDataset(df) :\n",
    "        \"\"\"Convert a DataFrame with features and labels into a PyTorch ConcatDataset\n",
    "    \n",
    "        Parameters\n",
    "        ----------\n",
    "        df : pandas.DataFrame\n",
    "            DataFrame containing columns 'data', 'Alpha', 'Beta', 'Gamma', 'Delta', 'Theta',\n",
    "            'Pearson Cross-Correlation Coefficient', and 'label'\n",
    "        Returns\n",
    "        -------\n",
    "        torch.utils.data.ConcatDataset\n",
    "            Concatenated PyTorch dataset containing data, additional features, and labels\n",
    "        \"\"\"\n",
    "        unique_dataset_indices = df['dataset_idx'].unique()\n",
    "        datasets = []\n",
    "\n",
    "        for dataset_idx in unique_dataset_indices:\n",
    "            subset_df = df[df['dataset_idx'] == dataset_idx]\n",
    "\n",
    "            data = [torch.from_numpy(np.array(d)) for d in subset_df['data']]\n",
    "            label = subset_df['label'].values\n",
    "            # Création d'un dictionnaire pour les fonctionnalités supplémentaires\n",
    "            additional_features = {\n",
    "            'alpha': [torch.from_numpy(np.array(a)) for a in df['Alpha']],\n",
    "            'beta': [torch.from_numpy(np.array(b)) for b in df['Beta']],\n",
    "            'gamma': [torch.from_numpy(np.array(g)) for g in df['Gamma']],\n",
    "            'delta': [torch.from_numpy(np.array(d)) for d in df['Delta']],\n",
    "            'theta': [torch.from_numpy(np.array(t)) for t in df['Theta']],\n",
    "            'pearson': [torch.tensor(c) for c in df['Pearson Cross-Correlation Coefficient']]\n",
    "            }\n",
    "\n",
    "            dataset = EpochsDatasetv2(data, additional_features, label)\n",
    "            datasets.append(dataset)\n",
    "\n",
    "        concatdataset = ConcatDataset(datasets)\n",
    "        \n",
    "        return concatdataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds = DataFrame_to_ConcatDataset(df_train)\n",
    "del(df_train) # we don't need it anymore --> delete to save some memory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If `additional_features=True` was chosen, we need to wrap the valid and test dataframes with additional features into Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_ds = DataFrame_to_ConcatDataset(valid_df)\n",
    "test_ds = DataFrame_to_ConcatDataset(test_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sanity check :\n",
    "\n",
    "If a resampling was performed, all the classes should be attributed a weight of 1. We compute the weights with the following :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Computing class weight\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "\n",
    "train_y = np.concatenate([ds.epochs_labels for ds in train_ds.datasets])\n",
    "class_weights = compute_class_weight('balanced', classes=np.unique(train_y), y=train_y)\n",
    "print(class_weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Creating the neural network\n",
    "\n",
    "In this section, we will define our ConvNet architecture. \n",
    "\n",
    "By default, we use the sleep staging architecture of Chambon et al. (2018), which looks something like this (adapted from Banville et al. 2020):\n",
    "\n",
    "![convnet](figures/convnet.png \"SleepStagerChambon2018\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The **input**, on the left, is a 30-s window of `C` channels. By default we have set `C` to 2 by selecting the 2 available EEG channels in Sleep Physionet above.\n",
    "\n",
    "The **output**, on the right, is a 5-dimensional vector where each dimension is matched to one of our 5 classes (W, N1, N2, N3 and R sleep stages).\n",
    "\n",
    "In between, we have a succession of convolutional layers, max pooling, and nonlinearities. The feature maps are finally flattened and passed through a fully-connected layer.\n",
    "\n",
    "We define the neural network in the following `torch.nn.Module` class:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "\n",
    "\n",
    "class SleepStagerChambon2018(nn.Module):\n",
    "    \"\"\"Sleep staging architecture from [1]_.\n",
    "    \n",
    "    Convolutional neural network for sleep staging described in [1]_.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    n_channels : int\n",
    "        Number of EEG channels.\n",
    "    sfreq : float\n",
    "        EEG sampling frequency.\n",
    "    n_conv_chs : int\n",
    "        Number of convolutional channels. Set to 8 in [1]_.\n",
    "    time_conv_size_s : float\n",
    "        Size of filters in temporal convolution layers, in seconds. Set to 0.5\n",
    "        in [1]_ (64 samples at sfreq=128).\n",
    "    max_pool_size_s : float\n",
    "        Max pooling size, in seconds. Set to 0.125 in [1]_ (16 samples at\n",
    "        sfreq=128).\n",
    "    n_classes : int\n",
    "        Number of classes.\n",
    "    input_size_s : float\n",
    "        Size of the input, in seconds.\n",
    "    dropout : float\n",
    "        Dropout rate before the output dense layer.\n",
    "        \n",
    "    References\n",
    "    ----------\n",
    "    .. [1] Chambon, S., Galtier, M. N., Arnal, P. J., Wainrib, G., &\n",
    "           Gramfort, A. (2018). A deep learning architecture for temporal sleep\n",
    "           stage classification using multivariate and multimodal time series.\n",
    "           IEEE Transactions on Neural Systems and Rehabilitation Engineering,\n",
    "           26(4), 758-769.\n",
    "    \"\"\"\n",
    "    def __init__(self, n_channels, sfreq, n_conv_chs=8, time_conv_size_s=0.5,\n",
    "                 max_pool_size_s=0.125, n_classes=5, input_size_s=30,\n",
    "                 dropout=0.25, len_additional_features=11):\n",
    "        super().__init__()\n",
    "\n",
    "        time_conv_size = int(time_conv_size_s * sfreq)\n",
    "        max_pool_size = int(max_pool_size_s * sfreq)\n",
    "        input_size = int(input_size_s * sfreq)\n",
    "        pad_size = time_conv_size // 2\n",
    "        self.n_channels = n_channels\n",
    "        if additional_features == False :\n",
    "            len_last_layer = self._len_last_layer(\n",
    "                n_channels, input_size, max_pool_size, n_conv_chs)\n",
    "        else :\n",
    "            len_last_layer = self._len_last_layer(\n",
    "                n_channels, input_size, max_pool_size, n_conv_chs, len_additional_features)\n",
    "            \n",
    "        if n_channels > 1:\n",
    "            self.spatial_conv = nn.Conv2d(1, n_channels, (n_channels, 1))\n",
    "\n",
    "        self.feature_extractor = nn.Sequential(\n",
    "            nn.Conv2d(\n",
    "                1, n_conv_chs, (1, time_conv_size), padding=(0, pad_size)),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d((1, max_pool_size)),\n",
    "            nn.Conv2d(\n",
    "                n_conv_chs, n_conv_chs, (1, time_conv_size),\n",
    "                padding=(0, pad_size)),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d((1, max_pool_size))\n",
    "        )\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(len_last_layer, n_classes)\n",
    "        )\n",
    "\n",
    "    if additional_features == False :\n",
    "        @staticmethod\n",
    "        def _len_last_layer(n_channels, input_size, max_pool_size, n_conv_chs):\n",
    "            return n_channels * (input_size // (max_pool_size ** 2)) * n_conv_chs\n",
    "        \n",
    "\n",
    "        def forward(self, x):\n",
    "            \"\"\"Forward pass.\n",
    "            \n",
    "            Parameters\n",
    "            ---------\n",
    "            x: torch.Tensor\n",
    "                Batch of EEG windows of shape (batch_size, n_channels, n_times).\n",
    "            \"\"\"\n",
    "            if self.n_channels > 1:\n",
    "                x = self.spatial_conv(x)\n",
    "                x = x.transpose(1, 2)\n",
    "\n",
    "            x = self.feature_extractor(x)\n",
    "            return self.fc(x.flatten(start_dim=1))\n",
    "    \n",
    "    else :\n",
    "        @staticmethod\n",
    "        def _len_last_layer(n_channels, input_size, max_pool_size, n_conv_chs, len_additional_features):\n",
    "            return (n_channels * (input_size // (max_pool_size ** 2)) * n_conv_chs) + len_additional_features\n",
    "        \n",
    "\n",
    "        def forward(self, eeg_data, additional_features):\n",
    "            \"\"\"Forward pass.\"\"\"\n",
    "            if self.n_channels > 1:\n",
    "                eeg_data = self.spatial_conv(eeg_data)\n",
    "                eeg_data = eeg_data.transpose(1, 2)\n",
    "                \n",
    "            eeg_data = self.feature_extractor(eeg_data)\n",
    "            eeg_data = eeg_data.flatten(start_dim=1)\n",
    "            # for the tensor size to go from ([128,11,1]) to ([128,11])\n",
    "            additional_features = additional_features.view(-1, 11)\n",
    "\n",
    "            # Concatenate EEG and additional features\n",
    "            x = torch.cat([eeg_data, additional_features], dim=1)\n",
    "            \n",
    "            return self.fc(x)\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We instantiate our ConvNet with the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sfreq = raws[0].info['sfreq']  # Sampling frequency\n",
    "n_channels = raws[0].info['nchan']  # Number of channels\n",
    "\n",
    "model = SleepStagerChambon2018(n_channels, sfreq, n_classes=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Depending on whether a CUDA-enabled GPU is available, we can move the model to the GPU and perform the training there. This can enable significant speed-ups, but is not strictly required for this tutorial."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Using device \\'{device}\\'.')\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Train and monitor network\n",
    "\n",
    "We are almost ready to finally train our ConvNet!\n",
    "\n",
    "We first need to define `DataLoader`s. `DataLoader` is a pytorch object that wraps a dataset and makes it easy to obtain batches of examples to feed to our neural network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# Create dataloaders\n",
    "train_batch_size = 128  # Important hyperparameter\n",
    "valid_batch_size = 256  # Can be made as large as what fits in memory; won't impact performance\n",
    "num_workers = 0  # Number of processes to use for the data loading process; 0 is the main Python process\n",
    "\n",
    "loader_train = DataLoader(\n",
    "    train_ds, batch_size=train_batch_size, shuffle=True, num_workers=num_workers)\n",
    "loader_valid = DataLoader(\n",
    "    valid_ds, batch_size=valid_batch_size, shuffle=False, num_workers=num_workers)\n",
    "loader_test = DataLoader(\n",
    "    test_ds, batch_size=valid_batch_size, shuffle=False, num_workers=num_workers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we define a few functions to carry out our training and validation loops:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import balanced_accuracy_score, cohen_kappa_score\n",
    "\n",
    "def _do_train(model, loader, optimizer, criterion, device, scheduler, metric, additional_features=additional_features):\n",
    "    # training loop\n",
    "    model.train()\n",
    "    \n",
    "    train_loss = np.zeros(len(loader))\n",
    "    y_pred_all, y_true_all = list(), list()\n",
    "     \n",
    "    if additional_features == True :\n",
    "        for idx_batch, data in enumerate(loader):\n",
    "            optimizer.zero_grad()\n",
    "            batch_x = data['eeg_data'].to(device=device, dtype=torch.float32)\n",
    "            batch_y = data['label'].to(device=device, dtype=torch.int64)\n",
    "            batch_alpha = data['additional_features']['alpha'].to(device=device, dtype=torch.float32)\n",
    "            batch_beta = data['additional_features']['beta'].to(device=device, dtype=torch.float32)\n",
    "            batch_gamma = data['additional_features']['gamma'].to(device=device, dtype=torch.float32)\n",
    "            batch_delta = data['additional_features']['delta'].to(device=device, dtype=torch.float32)\n",
    "            batch_theta = data['additional_features']['theta'].to(device=device, dtype=torch.float32)\n",
    "            batch_pearson = data['additional_features']['pearson'].to(device=device, dtype=torch.float32)\n",
    "            batch_pearson = batch_pearson.unsqueeze(1).unsqueeze(2)\n",
    "\n",
    "            additional_features = torch.cat([batch_alpha, batch_beta, batch_gamma, batch_delta, batch_theta, batch_pearson], dim=1)\n",
    "            output = model.forward(batch_x, additional_features)\n",
    "\n",
    "            loss = criterion(output, batch_y)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            y_pred_all.append(torch.argmax(output, axis=1).cpu().numpy())\n",
    "            y_true_all.append(batch_y.cpu().numpy())\n",
    "\n",
    "            train_loss[idx_batch] = loss.item()\n",
    "\n",
    "    else :\n",
    "        for idx_batch, (batch_x, batch_y) in enumerate(loader):\n",
    "            optimizer.zero_grad()\n",
    "            batch_x = batch_x.to(device=device, dtype=torch.float32)\n",
    "            batch_y = batch_y.to(device=device, dtype=torch.int64)\n",
    "\n",
    "            output = model(batch_x)\n",
    "            loss = criterion(output, batch_y)\n",
    "\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            y_pred_all.append(torch.argmax(output, axis=1).cpu().numpy())\n",
    "            y_true_all.append(batch_y.cpu().numpy())\n",
    "\n",
    "            train_loss[idx_batch] = loss.item()\n",
    "            \n",
    "    y_pred = np.concatenate(y_pred_all)\n",
    "    y_true = np.concatenate(y_true_all)\n",
    "    perf = metric(y_true, y_pred)\n",
    "    scheduler.step(np.mean(train_loss))\n",
    "    \n",
    "    return np.mean(train_loss), perf, balanced_accuracy_score(y_true, y_pred)\n",
    "        \n",
    "\n",
    "def _validate(model, loader, criterion, device, metric, additional_features=additional_features):\n",
    "    # validation loop\n",
    "    model.eval()\n",
    "    \n",
    "    val_loss = np.zeros(len(loader))\n",
    "    y_pred_all, y_true_all = list(), list()\n",
    "    with torch.no_grad():\n",
    "        if additional_features == True :\n",
    "            for idx_batch, data in enumerate(loader):\n",
    "                batch_x = data['eeg_data'].to(device=device, dtype=torch.float32)\n",
    "                batch_y = data['label'].to(device=device, dtype=torch.int64)\n",
    "                batch_alpha = data['additional_features']['alpha'].to(device=device, dtype=torch.float32)\n",
    "                batch_beta = data['additional_features']['beta'].to(device=device, dtype=torch.float32)\n",
    "                batch_gamma = data['additional_features']['gamma'].to(device=device, dtype=torch.float32)\n",
    "                batch_delta = data['additional_features']['delta'].to(device=device, dtype=torch.float32)\n",
    "                batch_theta = data['additional_features']['theta'].to(device=device, dtype=torch.float32)\n",
    "                batch_pearson = data['additional_features']['pearson'].to(device=device, dtype=torch.float32)\n",
    "                batch_pearson = batch_pearson.unsqueeze(1).unsqueeze(2)\n",
    "                additional_features = torch.cat([batch_alpha, batch_beta, batch_gamma, batch_delta, batch_theta, batch_pearson], dim=1)\n",
    "\n",
    "                output = model.forward(batch_x, additional_features)\n",
    "                loss = criterion(output, batch_y)\n",
    "                val_loss[idx_batch] = loss.item()\n",
    "                \n",
    "                y_pred_all.append(torch.argmax(output, axis=1).cpu().numpy())\n",
    "                y_true_all.append(batch_y.cpu().numpy())\n",
    "\n",
    "        else :\n",
    "            for idx_batch, (batch_x, batch_y) in enumerate(loader):\n",
    "                batch_x = batch_x.to(device=device, dtype=torch.float32)\n",
    "                batch_y = batch_y.to(device=device, dtype=torch.int64)\n",
    "                output = model.forward(batch_x)\n",
    "\n",
    "                loss = criterion(output, batch_y)\n",
    "                val_loss[idx_batch] = loss.item()\n",
    "                \n",
    "                y_pred_all.append(torch.argmax(output, axis=1).cpu().numpy())\n",
    "                y_true_all.append(batch_y.cpu().numpy())\n",
    "            \n",
    "    y_pred = np.concatenate(y_pred_all)\n",
    "    y_true = np.concatenate(y_true_all)\n",
    "    perf = metric(y_true, y_pred)\n",
    "\n",
    "    return np.mean(val_loss), perf, balanced_accuracy_score(y_true, y_pred)\n",
    "    \n",
    "\n",
    "def train(model, loader_train, loader_valid, optimizer, scheduler, criterion, n_epochs, \n",
    "          patience, device, metric=None):\n",
    "    \"\"\"Training function.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    model : instance of nn.Module\n",
    "        The model.\n",
    "    loader_train : instance of Sampler\n",
    "        The generator of EEG samples the model has to train on.\n",
    "        It contains n_train samples\n",
    "    loader_valid : instance of Sampler\n",
    "        The generator of EEG samples the model has to validate on.\n",
    "        It contains n_val samples. The validation samples are used to\n",
    "        monitor the training process and to perform early stopping\n",
    "    optimizer : instance of optimizer\n",
    "        The optimizer to use for training.\n",
    "    n_epochs : int\n",
    "        The maximum of epochs to run.\n",
    "    patience : int\n",
    "        The patience parameter, i.e. how long to wait for the\n",
    "        validation error to go down.\n",
    "    metric : None | callable\n",
    "        Metric to use to evaluate performance on the training and\n",
    "        validation sets. Defaults to balanced accuracy.\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    best_model : instance of nn.Module\n",
    "        The model that led to the best prediction on the validation\n",
    "        dataset.\n",
    "    history : list of dicts\n",
    "        Training history (loss, accuracy, etc.)\n",
    "    \"\"\"\n",
    "    best_valid_loss = np.inf\n",
    "    best_model = copy.deepcopy(model)\n",
    "    waiting = 0\n",
    "    history = list()\n",
    "    \n",
    "    if metric is None:\n",
    "        metric = balanced_accuracy_score\n",
    "        \n",
    "    print('epoch \\t train_loss \\t valid_loss \\t train_perf \\t valid_perf \\t train balanced accuracy \\t valid balanced accuracy')\n",
    "    print('--------------------------------------------------------------------------------------------------------------------------------')\n",
    "\n",
    "    for epoch in range(1, n_epochs + 1):\n",
    "        train_loss, train_perf, train_balanced_accuracy = _do_train(\n",
    "            model, loader_train, optimizer, criterion, device, scheduler, metric=metric)\n",
    "        valid_loss, valid_perf, valid_balanced_accuracy = _validate(\n",
    "            model, loader_valid, criterion, device, metric=metric)\n",
    "        history.append(\n",
    "            {'epoch': epoch, \n",
    "             'train_loss': train_loss, 'valid_loss': valid_loss,\n",
    "             'train_perf': train_perf, 'valid_perf': valid_perf})\n",
    "        \n",
    "        print(f'{epoch} \\t {train_loss:0.4f} \\t {valid_loss:0.4f} '\n",
    "              f'\\t {train_perf:0.4f} \\t {valid_perf:0.4f} \\t {train_balanced_accuracy:0.4f} \\t\\t\\t {valid_balanced_accuracy:0.4f}')\n",
    "\n",
    "        # model saving\n",
    "        if valid_loss < best_valid_loss:\n",
    "            print(f'best val loss {best_valid_loss:.4f} -> {valid_loss:.4f}')\n",
    "            best_valid_loss = valid_loss\n",
    "            best_model = copy.deepcopy(model)\n",
    "            waiting = 0\n",
    "        else:\n",
    "            waiting += 1\n",
    "\n",
    "        # model early stopping\n",
    "        if waiting >= patience:\n",
    "            print(f'Stop training at epoch {epoch}')\n",
    "            print(f'Best val loss : {best_valid_loss:.4f}')\n",
    "            break\n",
    "\n",
    "    return best_model, history"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Two critical pieces of the training process are the **optimizer** and the **criterion**.\n",
    "\n",
    "* The **optimizer** implements the parameter update procedure. Here, we use `Adam`, a popular adaptive gradient descent optimizer for deep neural networks.\n",
    "* The **criterion**, or loss function, is used to measure how well the neural network performs on an example. Here, we use the standard multiclass cross-entropy loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn import CrossEntropyLoss\n",
    "from torch.optim import Adam\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "\n",
    "optimizer = Adam(model.parameters(), lr=1e-3, weight_decay=0)\n",
    "scheduler = ReduceLROnPlateau(optimizer, mode='min', factor=0.2, patience=3, verbose=True)\n",
    "criterion = CrossEntropyLoss(weight=torch.Tensor(class_weights).to(device))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now launch our training loop. The maxmium number of training epochs (or \"passes\" through the training set) is set with `n_epochs`. The `patience` hyperparameter controls how many epochs we will wait for before stopping the training process if there is no improvement on the validation set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_epochs = 15\n",
    "patience = 5\n",
    "\n",
    "best_model, history = train(\n",
    "    model, loader_train, loader_valid, optimizer, criterion, n_epochs, patience, \n",
    "    device, metric=cohen_kappa_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we visualize the results of our training.\n",
    "\n",
    "First, the training curves show how the loss and accuracy improved across training epochs. We use [Cohen's kappa](https://scikit-learn.org/stable/modules/model_evaluation.html#cohen-kappa) (instead of the standard accuracy) to better reflect performance under class imbalance and allow comparison with results from the literature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualizing the learning curves\n",
    "\n",
    "history_df = pd.DataFrame(history)\n",
    "ax1 = history_df.plot(x='epoch', y=['train_loss', 'valid_loss'], marker='o')\n",
    "ax1.set_ylabel('Loss')\n",
    "ax2 = history_df.plot(x='epoch', y=['train_perf', 'valid_perf'], marker='o')\n",
    "ax2.set_ylabel('Cohen\\'s kappa')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We expect the loss to decrease and the Cohen's kappa to increase as more and more training epochs are performed.\n",
    "\n",
    "We also measure the performance on the test set, which was not seen during training. This gives us a better estimate of the generalization performance of our ConvNet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute test performance\n",
    "\n",
    "best_model.eval()\n",
    "\n",
    "y_pred_all, y_true_all = list(), list()\n",
    "for batch_x, batch_y in loader_test:\n",
    "    batch_x = batch_x.to(device=device, dtype=torch.float32)\n",
    "    batch_y = batch_y.to(device=device, dtype=torch.int64)\n",
    "    output = model.forward(batch_x)\n",
    "    y_pred_all.append(torch.argmax(output, axis=1).cpu().numpy())\n",
    "    y_true_all.append(batch_y.cpu().numpy())\n",
    "    \n",
    "y_pred = np.concatenate(y_pred_all)\n",
    "y_true = np.concatenate(y_true_all)\n",
    "rec_ids = np.concatenate(  # indicates which recording each example comes from\n",
    "    [[i] * len(ds) for i, ds in enumerate(test_ds.datasets)])\n",
    "\n",
    "test_bal_acc = balanced_accuracy_score(y_true, y_pred)\n",
    "test_kappa = cohen_kappa_score(y_true, y_pred)\n",
    "\n",
    "print(f'Test balanced accuracy: {test_bal_acc:0.3f}')\n",
    "print(f'Test Cohen\\'s kappa: {test_kappa:0.3f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For our 5-class problem, chance-level would correspond to 20% balanced accuracy or a Cohen's kappa of 0.0.\n",
    "\n",
    "To get a sense of what is possible, a recent model achieved a kappa of 0.814 on the Sleep Physionet data using a single EEG channel (10-fold cross-validation):\n",
    "\n",
    "> Phan, H., Chén, O. Y., Koch, P., Mertins, A., & De Vos, M. (2020). Xsleepnet: Multi-view sequential model for automatic sleep staging. arXiv preprint arXiv:2007.05492"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12. Visualizing results\n",
    "\n",
    "We further inspect the results in this section.\n",
    "\n",
    "We start by looking at the [confusion matrix](https://en.wikipedia.org/wiki/Confusion_matrix), which shows which classes were easier or more difficult to classify for our ConvNet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "def plot_normalized_confusion_matrix(conf_mat, classes_mapping):\n",
    "    ticks = list(classes_mapping.keys())\n",
    "    tick_labels = classes_mapping.values()\n",
    "\n",
    "    # Normalize the confusion matrix\n",
    "    row_sums = conf_mat.sum(axis=1)\n",
    "    normalized_conf_mat = conf_mat / row_sums[:, np.newaxis]\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(6, 6))\n",
    "    im = ax.imshow(normalized_conf_mat, cmap='Reds', vmin=0, vmax=1)  # Set vmin and vmax to ensure values are between 0 and 1\n",
    "\n",
    "    ax.set_yticks(ticks)\n",
    "    ax.set_yticklabels(tick_labels)\n",
    "    ax.set_xticks(ticks)\n",
    "    ax.set_xticklabels(tick_labels)\n",
    "    ax.set_ylabel('True label')\n",
    "    ax.set_xlabel('Predicted label')\n",
    "    ax.set_title('Normalized Confusion matrix')\n",
    "\n",
    "    for i in range(len(ticks)):\n",
    "        for j in range(len(ticks)):\n",
    "            text = ax.text(\n",
    "                j, i, f'{normalized_conf_mat[i, j]:.2f}', ha='center', va='center', color='k')\n",
    "\n",
    "    fig.colorbar(im, ax=ax, fraction=0.05, label='Normalized Value')\n",
    "    fig.tight_layout()\n",
    "\n",
    "    return fig, ax\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conf_mat = confusion_matrix(y_true, y_pred)\n",
    "plot_normalized_confusion_matrix(conf_mat, classes_mapping);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What kind of mistakes does the ConvNet seem to make? Is there a class that's often mistaken for another one?\n",
    "\n",
    "We can also visualize the predictions on a recording basis. This visualization is known as a \"[hypnogram](https://en.wikipedia.org/wiki/Hypnogram)\". A hypnogram shows the evolution of sleep stages across an overnight recording."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The ConcNet seems to be mistaking the stages W, N1 and R. We will try to see why by projecting them in the 2D ACP space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot hypnogram for one recording\n",
    "\n",
    "mask = rec_ids == 0  # pick a recording number\n",
    "\n",
    "t = np.arange(len(y_true[mask])) * 30 / 3600\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(12, 3))\n",
    "ax.plot(t, y_true[mask], label='True')\n",
    "ax.plot(t, y_pred[mask], alpha=0.7, label='Predicted')\n",
    "ax.set_yticks([0, 1, 2, 3, 4])\n",
    "ax.set_yticklabels(['W', 'N1', 'N2', 'N3', 'R'])\n",
    "ax.set_xlabel('Time (h)')\n",
    "ax.set_title('Hypnogram')\n",
    "ax.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Do the predictions of the ConvNet follow the groundtruth hypnogram? Is there any structure in the way mistakes are made?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Going further\n",
    "\n",
    "You've now covered all the material for this tutorial!\n",
    "\n",
    "To test what you have learned, we recommend you identify a few key elements in the pipeline shown above, and play with them to try to improve the performance of your ConvNet. Here are a few ideas to get you started:\n",
    "- Increasing the training set size\n",
    "- Improving the architecture (you can look at recent sleep staging literature, or follow your intuition!)\n",
    "- Optimizing the training hyperparameters (learning rate, batch size, etc.)\n",
    "\n",
    "Good luck! :)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
